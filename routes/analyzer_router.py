"""
SOC Alert Analyzer Router
FastAPI router for AI-powered security alert analysis
"""

import pandas as pd
from datetime import datetime
from typing import Optional, Dict, List, Any
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field

# Import your backend analyzer
from backend.analyzer_backend import SecurityAlertAnalyzerCrew
from backend.soc_analyzer import IntelligentSOCAnalyzer

# Create router
router = APIRouter()

# Global cache for analyzers
_soc_analyzer: Optional[IntelligentSOCAnalyzer] = None
_alert_analyzer: Optional[SecurityAlertAnalyzerCrew] = None
_cache_timestamp = None


# ============================================================================
# Pydantic Models (Request/Response Schemas)
# ============================================================================


class RuleSuggestionRequest(BaseModel):
    """Request model for rule suggestions"""

    query: str = Field(
        ..., description="Search query for rule suggestions", min_length=1
    )
    top_k: int = Field(5, ge=1, le=20, description="Number of suggestions to return")


class RuleSuggestion(BaseModel):
    """Individual rule suggestion"""

    rule: str
    score: float
    incident_count: int
    match_type: str = "text_similarity"


class RuleSuggestionsResponse(BaseModel):
    """Response model for rule suggestions"""

    success: bool
    query: str
    total_found: int
    suggestions: List[RuleSuggestion]
    timestamp: str


class AnalyzeAlertRequest(BaseModel):
    """Request model for alert analysis"""

    alert_name: str = Field(..., description="Alert/Rule name to analyze", min_length=1)


class AnalyzeAlertResponse(BaseModel):
    """Response model for alert analysis"""

    success: bool
    alert_name: str
    analysis: str
    timestamp: str


class HistoricalDataRequest(BaseModel):
    """Request model for historical data"""

    rule_name: str = Field(..., description="Rule name to get historical data")


class HistoricalDataResponse(BaseModel):
    """Response model for historical data"""

    success: bool
    rule_name: str
    total_incidents: int
    data: List[Dict[str, Any]]
    timestamp: str


class SystemStatsResponse(BaseModel):
    """Response model for system statistics"""

    success: bool
    total_records: int
    unique_rules: int
    data_sources: int
    timestamp: str


class HealthResponse(BaseModel):
    """Response model for health check"""

    status: str
    timestamp: str
    soc_analyzer_loaded: bool
    alert_analyzer_loaded: bool
    cache_timestamp: Optional[str] = None
    total_records: int = 0


class LoadDataResponse(BaseModel):
    """Response model for load data endpoint"""

    success: bool
    total_records: int
    unique_rules: int
    message: str
    timestamp: str


class ErrorResponse(BaseModel):
    """Standard error response"""

    success: bool = False
    error: str
    detail: Optional[str] = None


# ============================================================================
# Helper Functions
# ============================================================================


def get_analyzers(force_reload: bool = False):
    """
    Load and cache analyzer instances

    Args:
        force_reload: Force reload analyzers

    Returns:
        Tuple of (soc_analyzer, alert_analyzer)

    Raises:
        HTTPException: If analyzer initialization fails
    """
    global _soc_analyzer, _alert_analyzer, _cache_timestamp

    if force_reload or _soc_analyzer is None or _alert_analyzer is None:
        try:
            # Initialize SOC Analyzer
            soc_analyzer = IntelligentSOCAnalyzer(
                data_directory="data", ollama_model="qwen2.5:0.5b"
            )

            if not soc_analyzer.load_and_process_data():
                raise Exception("Failed to load SOC data")

            # Initialize Alert Analyzer
            alert_analyzer = SecurityAlertAnalyzerCrew()

            _soc_analyzer = soc_analyzer
            _alert_analyzer = alert_analyzer
            _cache_timestamp = datetime.now()

            print(f"âœ… Loaded analyzers with {len(_soc_analyzer.df)} records")
            return _soc_analyzer, _alert_analyzer

        except Exception as e:
            raise HTTPException(
                status_code=500, detail=f"Error initializing analyzers: {str(e)}"
            )

    return _soc_analyzer, _alert_analyzer


# ============================================================================
# API Endpoints
# ============================================================================


@router.get(
    "/health",
    response_model=HealthResponse,
    summary="Health check",
    description="Check if the SOC analyzer service is healthy and analyzers are loaded",
)
async def health_check():
    """
    Health check endpoint for SOC analyzer service

    Returns status, analyzer loading state, and cache information
    """
    return HealthResponse(
        status="healthy",
        timestamp=datetime.now().isoformat(),
        soc_analyzer_loaded=_soc_analyzer is not None,
        alert_analyzer_loaded=_alert_analyzer is not None,
        cache_timestamp=_cache_timestamp.isoformat() if _cache_timestamp else None,
        total_records=(
            len(_soc_analyzer.df)
            if _soc_analyzer and _soc_analyzer.df is not None
            else 0
        ),
    )


@router.post(
    "/load-data",
    response_model=LoadDataResponse,
    summary="Load SOC data",
    description="Load or reload SOC analyzer data from the data directory",
)
async def load_data():
    """
    Load or reload SOC analyzer data from files

    This endpoint forces a reload of all data and reinitializes analyzers.
    Use this when new data files are added or existing files are updated.

    Returns:
        LoadDataResponse with success status and statistics
    """
    try:
        soc_analyzer, _ = get_analyzers(force_reload=True)

        total_records = len(soc_analyzer.df) if soc_analyzer.df is not None else 0
        unique_rules = (
            soc_analyzer.df["RULE"].nunique()
            if "RULE" in soc_analyzer.df.columns
            else 0
        )

        return LoadDataResponse(
            success=True,
            total_records=total_records,
            unique_rules=unique_rules,
            message=f"Successfully loaded {total_records} records with {unique_rules} unique rules",
            timestamp=datetime.now().isoformat(),
        )

    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Unexpected error loading data: {str(e)}"
        )


@router.post(
    "/suggestions",
    response_model=RuleSuggestionsResponse,
    summary="Get rule suggestions",
    description="Search for rule suggestions based on query with intelligent matching",
)
async def get_rule_suggestions(request: RuleSuggestionRequest):
    """
    Get rule suggestions based on search query

    This endpoint searches across rule names and returns ranked suggestions
    using intelligent text similarity and rule number matching.

    Args:
        request: RuleSuggestionRequest with query and top_k parameters

    Returns:
        RuleSuggestionsResponse with matching rule suggestions
    """
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="Query parameter cannot be empty")

    # Get cached analyzer
    soc_analyzer, _ = get_analyzers()

    if soc_analyzer.df is None or soc_analyzer.df.empty:
        raise HTTPException(
            status_code=500,
            detail="No SOC data available. Please load data first using /load-data endpoint.",
        )

    # Get suggestions
    try:
        suggestions = soc_analyzer.get_rule_suggestions(
            request.query, top_k=request.top_k
        )
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error getting suggestions: {str(e)}"
        )

    # Convert to Pydantic models
    suggestion_models = [
        RuleSuggestion(
            rule=s["rule"],
            score=s["score"],
            incident_count=s["incident_count"],
            match_type=s.get("match_type", "text_similarity"),
        )
        for s in suggestions
    ]

    return RuleSuggestionsResponse(
        success=True,
        query=request.query,
        total_found=len(suggestion_models),
        suggestions=suggestion_models,
        timestamp=datetime.now().isoformat(),
    )


@router.post(
    "/analyze",
    response_model=AnalyzeAlertResponse,
    summary="Analyze alert with AI",
    description="Generate comprehensive AI-powered threat intelligence analysis for an alert",
)
async def analyze_alert(request: AnalyzeAlertRequest):
    """
    Analyze alert with AI-powered threat intelligence

    This endpoint generates comprehensive analysis including:
    - Technical threat breakdown
    - MITRE ATT&CK technique mapping
    - Threat actor intelligence
    - Business impact assessment

    Args:
        request: AnalyzeAlertRequest with alert_name

    Returns:
        AnalyzeAlertResponse with generated analysis
    """
    if not request.alert_name.strip():
        raise HTTPException(status_code=400, detail="Alert name cannot be empty")

    # Get cached analyzer
    _, alert_analyzer = get_analyzers()

    try:
        # Generate analysis (this may take time due to LLM processing)
        analysis = alert_analyzer.analyze_alert(request.alert_name)

        return AnalyzeAlertResponse(
            success=True,
            alert_name=request.alert_name,
            analysis=analysis,
            timestamp=datetime.now().isoformat(),
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error analyzing alert: {str(e)}")


@router.post(
    "/historical-data",
    response_model=HistoricalDataResponse,
    summary="Get historical incident data",
    description="Retrieve historical incident data for a specific rule",
)
async def get_historical_data(request: HistoricalDataRequest):
    """
    Get historical incident data for a rule

    This endpoint retrieves all historical incidents associated with
    a specific rule name from the SOC tracker data.

    Args:
        request: HistoricalDataRequest with rule_name

    Returns:
        HistoricalDataResponse with historical incident data
    """
    if not request.rule_name.strip():
        raise HTTPException(status_code=400, detail="Rule name cannot be empty")

    # Get cached analyzer
    soc_analyzer, _ = get_analyzers()

    try:
        # Filter data for specific rule
        matching_data = soc_analyzer.df[
            soc_analyzer.df["RULE"] == request.rule_name
        ].copy()

        if matching_data.empty:
            raise HTTPException(
                status_code=404,
                detail=f"No historical data found for rule: {request.rule_name}",
            )

        # Convert to dict records with proper serialization
        records = []
        for _, row in matching_data.iterrows():
            record = {}
            for key, value in row.items():
                if pd.isna(value):
                    record[key] = None
                elif isinstance(value, (pd.Timestamp, datetime)):
                    record[key] = value.isoformat()
                else:
                    record[key] = str(value)
            records.append(record)

        return HistoricalDataResponse(
            success=True,
            rule_name=request.rule_name,
            total_incidents=len(records),
            data=records,
            timestamp=datetime.now().isoformat(),
        )

    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving historical data: {str(e)}"
        )


@router.get(
    "/stats",
    response_model=SystemStatsResponse,
    summary="Get system statistics",
    description="Get overall system statistics including record counts and unique rules",
)
async def get_system_stats():
    """
    Get system statistics

    Returns overall statistics about the SOC data including
    total records, unique rules, and data sources.

    Returns:
        SystemStatsResponse with system statistics
    """
    # Get cached analyzer
    soc_analyzer, _ = get_analyzers()

    if soc_analyzer.df is None or soc_analyzer.df.empty:
        raise HTTPException(
            status_code=500, detail="No SOC data available. Please load data first."
        )

    try:
        total_records = len(soc_analyzer.df)
        unique_rules = (
            soc_analyzer.df["RULE"].nunique()
            if "RULE" in soc_analyzer.df.columns
            else 0
        )
        data_sources = (
            soc_analyzer.df["source_file"].nunique()
            if "source_file" in soc_analyzer.df.columns
            else 0
        )

        return SystemStatsResponse(
            success=True,
            total_records=total_records,
            unique_rules=unique_rules,
            data_sources=data_sources,
            timestamp=datetime.now().isoformat(),
        )

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error getting system stats: {str(e)}"
        )
